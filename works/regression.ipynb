{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys, os\n",
    "this_path = '/home/ibi/Documents/GitHub/mas291-project/'\n",
    "sys.path.append(this_path)\n",
    "os.chdir(this_path)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago_data = pd.read_csv('data/cleaned/new_york_sample.csv')\n",
    "chicago_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will choose area for the input and price for the output. So $x$ = area and $y$ = price. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = chicago_data['area'].values\n",
    "y = chicago_data['price'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatter plot of the data is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5, color='grey', zorder=0)\n",
    "plt.scatter(x, y, alpha=0.8, color='orange', zorder=3, marker='x', linewidths=0.7)  # Adjusted line width with linewidths=0.5\n",
    "plt.title('Scatter plot for area and price of sold houses in Chicago')\n",
    "plt.xlabel('Area')\n",
    "plt.ylabel('Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatter plot shows that the price of the house increases with the area of the house. But we need to know whether they are really related or not, so we can calculate the correlation coefficient. The correlation coefficient is a value that ranges from -1 to 1. The closer the value is to 1, the stronger the positive correlation. The closer the value is to -1, the stronger the negative correlation. The closer the value is to 0, the weaker the correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the correlation coefficient, we will follow these steps:\n",
    "1. Calculate the mean of $x$ and $y$.\n",
    "2. Calculate the deviation from the mean for each $x$ and $y$.\n",
    "3. Calculate the covariance of $x$ and $y$.\n",
    "4. Calculate the standard deviation of $x$ and $y$.\n",
    "5. Calculate the correlation coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Calculate the mean of $x$ and $y$.\n",
    "$$\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i$$\n",
    "$$\\bar{y} = \\frac{1}{n} \\sum_{i=1}^{n} y_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = len(x)\n",
    "# sum_x = sum(x)\n",
    "# sum_y = sum(y)\n",
    "# mean_x = sum_x / n\n",
    "# mean_y = sum_y / n\n",
    "\n",
    "mean_x = np.mean(x).round(2)\n",
    "mean_y = np.mean(y).round(2)\n",
    "\n",
    "print(f\"Mean of x: {mean_x:.2f}\")\n",
    "print(f\"Mean of y: {mean_y:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Calculate the deviation from the mean for each $x$ and $y$, ($x_i - \\bar{x}$) and ($y_i - \\bar{y}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deviation_x = x - mean_x\n",
    "deviation_y = y - mean_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Calculate the covariance of $x$ and $y$.\n",
    "$$cov(x, y) = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# covariance = sum(deviation_x * deviation_y) / n\n",
    "\n",
    "covariance = np.cov(x, y)[0][1]\n",
    "\n",
    "print(f\"Covariance: {covariance:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Calculate the standard deviation of $x$ and $y$.\n",
    "$$\\sigma_x = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})^2}$$\n",
    "$$\\sigma_y = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\bar{y})^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# std_deviation_x = (sum(deviation_x ** 2) / n) ** 0.5\n",
    "# std_deviation_y = (sum(deviation_y ** 2) / n) ** 0.5\n",
    "\n",
    "std_deviation_x = np.std(x).round(2)\n",
    "std_deviation_y = np.std(y).round(2)\n",
    "\n",
    "print(f\"Standard deviation of x: {std_deviation_x:.2f}\")\n",
    "print(f\"Standard deviation of y: {std_deviation_y:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Calculate the correlation coefficient.\n",
    "$$r = \\frac{cov(x, y)}{\\sigma_x \\sigma_y}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation = covariance / (std_deviation_x * std_deviation_y)\n",
    "\n",
    "correlation = np.corrcoef(x, y)[0][1]\n",
    "\n",
    "print(f\"Correlation: {correlation:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation coefficient of the area and price is 0.69, indicating a strong positive correlation between the two variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the regression line, we assume that the relationship between the area and price is linear. The regression line is given by:\n",
    "$$ \\text{price} = w_\\text{area} \\times \\text{area} + b$$\n",
    "where $w_{area}$ is the weight of the area and $b$ is the bias.\n",
    "\n",
    "The weight determines the influence of the area on the price, and the bias is the price when the area is zero.\n",
    "\n",
    "Fitting the regression line to the data means that we agree on some measure of error. The most common measure of error is the squared error. The squared error is the square of the difference between the actual price and the predicted price. The goal is to minimize the squared error. Let $\\widehat{\\text{price}}$ be the predicted price. The squared error is given by:\n",
    "$$ \\text{error} = \\dfrac{1}{2}(\\widehat{\\text{price}} - \\text{price})^2$$\n",
    "\n",
    "The square of the difference is used to make the error positive, and also to penalize large errors more than small errors. The factor of $\\dfrac{1}{2}$ is used to make the derivative of the error simpler.\n",
    "\n",
    "To measure the error of the regression line, we use the mean squared error (MSE). The MSE is the average of the squared errors. The MSE is given by:\n",
    "$$ \\text{MSE} = \\dfrac{1}{n} \\sum_{i=1}^{n} \\dfrac{1}{2}(\\widehat{\\text{price}_i} - \\text{price}_i)^2$$\n",
    "\n",
    "The goal is to find the weight and bias that minimize the MSE. To do this, we use the gradient descent algorithm. The gradient descent algorithm is an optimization algorithm that minimizes the error by updating the weight and bias in the opposite direction of the gradient of the error. In each iteration, we update the weight and bias using the following formulas:\n",
    "$$ w_{area} = w_{area} - \\alpha \\dfrac{\\partial \\text{MSE}}{\\partial w_{area}}$$\n",
    "$$ b = b - \\alpha \\dfrac{\\partial \\text{MSE}}{\\partial b}$$\n",
    "where $\\alpha$ is the learning rate, which determines the step size of the update.\n",
    "\n",
    "The partial derivative of the MSE with respect to the weight and bias is given by:\n",
    "$$ \\dfrac{\\partial \\text{MSE}}{\\partial w_{area}} = -\\dfrac{1}{n} \\sum_{i=1}^{n} (price_i - \\widehat{price}_i) \\times area_i$$\n",
    "$$ \\dfrac{\\partial \\text{MSE}}{\\partial b} = -\\dfrac{1}{n} \\sum_{i=1}^{n} (price_i - \\widehat{price}_i)$$\n",
    "\n",
    "The gradient descent algorithm is an iterative algorithm that updates the weight and bias until the error converges to a minimum. The algorithm is given by:\n",
    "1. Initialize the weight and bias to random values.\n",
    "2. Calculate the predicted price using the regression line.\n",
    "3. Calculate the error using the MSE.\n",
    "4. Update the weight and bias using the gradient descent algorithm.\n",
    "5. Repeat steps 2-4 until the error converges to a minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "len(x_train), len(y_train), len(x_test), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the input features\n",
    "x_train_mean = np.mean(x_train)\n",
    "x_train_std = np.std(x_train)\n",
    "x_train_normalized = (x_train - x_train_mean) / x_train_std\n",
    "\n",
    "y_train_mean = np.mean(y_train)\n",
    "y_train_std = np.std(y_train)\n",
    "y_train_normalized = (y_train - y_train_mean) / y_train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x_train_normalized, y_train_normalized, learning_rate=0.001, iterations=10000, w_init=0.0, b_init=0.0):\n",
    "    w = w_init\n",
    "    b = b_init\n",
    "\n",
    "    w_history = [w]\n",
    "    b_history = [b]\n",
    "    loss_history = []\n",
    "\n",
    "    def compute_loss(X, y, w, b):\n",
    "        return np.mean((w * X + b - y) ** 2)\n",
    "\n",
    "    def compute_gradient(X, y, w, b):\n",
    "        m = len(y)\n",
    "        y_pred = w * X + b\n",
    "        error = y_pred - y\n",
    "        dw = np.sum(x_train_normalized * error) / m\n",
    "        db = np.sum(error) / m\n",
    "        return dw, db\n",
    "\n",
    "    initial_loss = compute_loss(x_train_normalized, y_train_normalized, w, b)\n",
    "    loss_history.append(initial_loss)\n",
    "\n",
    "    for i in range(iterations):\n",
    "        dw, db = compute_gradient(x_train_normalized, y_train_normalized, w, b)\n",
    "        w -= learning_rate * dw\n",
    "        b -= learning_rate * db\n",
    "        w_history.append(w)\n",
    "        b_history.append(b)\n",
    "        loss = compute_loss(x_train_normalized, y_train_normalized, w, b)\n",
    "        loss_history.append(loss)\n",
    "\n",
    "    return w, b, w_history, b_history, loss_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_normalized = (x_train - x_train.mean()) / x_train.std()\n",
    "y_train_normalized = (y_train - y_train.mean()) / y_train.std()\n",
    "\n",
    "w_area, b, w_history, b_history, loss_history = gradient_descent(x_train_normalized, y_train_normalized)\n",
    "\n",
    "w_area_original = w_area * y_train.std() / x_train.std()\n",
    "b_original = b * y_train.std() + y_train.mean() - w_area_original * x_train.mean()\n",
    "\n",
    "print(f\"Weight for area (original scale): {w_area_original:.2f}\")\n",
    "print(f\"Bias (original scale): {b_original:.2f}\")\n",
    "print(f\"Final model: y = {w_area_original:.2f} * x + {b_original:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regression line is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train_normalized = w_area * x_train_normalized + b\n",
    "y_pred_train = y_pred_train_normalized * y_train_std + y_train_mean\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5, color='grey', zorder=0)\n",
    "plt.scatter(x_train, y_train, alpha=0.8, s=8, color='orange', edgecolor='none', zorder=3, label='Training data')\n",
    "plt.plot(x_train, y_pred_train, color='black', linewidth=1, zorder=2, label='Regression line')\n",
    "plt.title('Linear regression line for area and price of sold houses in Chicago')\n",
    "plt.xlabel('Area')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the test data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5, color='grey', zorder=0)\n",
    "plt.scatter(x_test, y_test, alpha=0.8, s=8, color='orange', edgecolor='none', zorder=3, label='Test data')\n",
    "plt.plot(x_train, y_pred_train, color='black', linewidth=1, zorder=2, label='Regression line')\n",
    "plt.title('Linear regression line for area and price of sold houses in Chicago')\n",
    "plt.xlabel('Area')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the model is calculated using the coefficient of determination, $R^2$. The coefficient of determination is a value that ranges from 0 to 1. The closer the value is to 1, the better the model fits the data. The coefficient of determination is given by:\n",
    "$$ R^2 = 1 - \\dfrac{\\sum_{i=1}^{n} (price_i - \\widehat{price}_i)^2}{\\sum_{i=1}^{n} (price_i - \\overline{price})^2}$$\n",
    "where $\\overline{price}$ is the mean of the price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_normalized = w_area * (x_test - x_train_mean) / x_train_std + b\n",
    "y_pred_test = y_pred_test_normalized * y_train_std + y_train_mean\n",
    "\n",
    "ssr = np.sum((y_test - y_pred_test) ** 2)\n",
    "sst = np.sum((y_test - np.mean(y_test)) ** 2)\n",
    "r2 = 1 - ssr / sst\n",
    "\n",
    "print(f\"R2: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficient of determination of the model is 0.53, indicating that the model explains 53% of the variance in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand what the model is doing, we can plot the gradient descent surface. The gradient descent surface shows the error as a function of the weight and bias. The goal is to find the weight and bias that minimize the error. The gradient descent surface is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_grid, b_grid = np.meshgrid(\n",
    "    np.linspace(min(w_history) - 0.5, max(w_history) + 0.5, 100),\n",
    "    np.linspace(min(b_history) - 0.5, max(b_history) + 0.5, 100)\n",
    ")\n",
    "Z = np.array([\n",
    "    np.mean((w * x_train_normalized + b - y_train_normalized) ** 2)\n",
    "    for w, b in zip(np.ravel(w_grid), np.ravel(b_grid))\n",
    "]).reshape(w_grid.shape)\n",
    "\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(w_grid, b_grid, Z, alpha=0.6, cmap='viridis')\n",
    "ax.plot(w_history, b_history, [np.mean((w * x_train_normalized + b - y_train_normalized) ** 2) for w, b in zip(w_history, b_history)], 'r-o')\n",
    "ax.set_xlabel('Weight (w)')\n",
    "ax.set_ylabel('Bias (b)')\n",
    "ax.set_zlabel('Loss')\n",
    "ax.set_title('Gradient Descent on Loss Surface')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(len(loss_history)), loss_history, 'b-o', markersize=4)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Reduction Over Iterations')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mas291project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
